---
title: Incident & Topology Integration
description: "How incidents use infrastructure topology for blast radius analysis and impact visualization"
icon: diagram-project
---

CloudThinker incidents don't exist in isolation. When a service fails, everything downstream fails too—your incident management must understand these dependencies. The Incident & Topology integration connects infrastructure visualization with incident investigation, automatically calculating impact, identifying affected services, and providing context for AI-powered root cause analysis.

---

## Overview

When you connect infrastructure topology to incidents, CloudThinker:

- **Auto-detects affected services** - Resolves incident service names to topology nodes
- **Calculates blast radius** - Shows primary failures and dependent cascading failures (1-hop neighbors)
- **Visualizes impact** - Colors topology nodes by incident severity, highlighting what's broken
- **Provides RCA context** - Agents investigate with full infrastructure dependency knowledge
- **Enables filtering** - Users can focus on affected services using "Affected Only" mode

---

## Architecture

### Data Model

Incidents store two topology-related fields:

```typescript
// Primary topology view for this incident
topology_view_id: UUID | null

// Manually pinned node IDs (fallback if no topology view selected)
pinned_node_ids: UUID[]
```

And metadata structure for blast radius:

```typescript
metadata: {
  blast_radius: {
    node_count?: number           // Count of affected nodes
    edge_count?: number           // Count of edges between affected nodes
    affected_domains?: string[]   // Service domains impacted
    affected_node_ids: UUID[]     // Array of topology node UUIDs
  }
}
```

### Service-to-Node Resolution

When agents identify affected services during investigation, CloudThinker automatically resolves them to topology nodes using a two-stage matching strategy:

**Stage 1: External ID Matching** (Exact)
- Matches against `GraphNode.external_id` (typically AWS ARNs)
- Example: `arn:aws:ec2:us-east-1:123456789:instance/i-abc123` → node UUID
- Fastest resolution for cloud resources with stable identifiers

**Stage 2: Name Pattern Matching** (Fuzzy)
- Matches against `GraphNode.name` using case-insensitive pattern matching (ILIKE)
- Example: "auth-service" → matches node named "auth-service-prod"
- Fallback for services without external IDs

**Batch Optimization:**
Both strategies execute as batch queries for performance:
```
SELECT node_id FROM graph_nodes
WHERE workspace_id = ? AND (
  external_id IN (?, ?, ...) OR
  name ILIKE ANY (?, ?, ...)
)
```

---

## How It Works

### Phase 1: Incident Creation

When an incident is created (manually, via API, or from webhook):

```
1. Incident created with optional topology_view_id
   └─ If not provided, defaults to workspace config

2. Auto-detect topology
   ├─ Extract service names from incident
   ├─ Query topology for matching nodes
   ├─ Update pinned_node_ids[]
   └─ Store matched services in metadata

3. Triggered RCA (if configured)
   └─ RCA agent receives incident with topology context
```

Example:

```json
{
  "title": "Auth Service Degradation",
  "severity": "high",
  "affected_services": ["auth-service", "auth-db"],
  "topology_view_id": "550e8400-e29b-41d4-a716-446655440000"
}
```

**Result:**
- System queries topology for "auth-service" and "auth-db"
- Finds matching nodes (UUID: abc123, UUID: def456)
- Stores in `pinned_node_ids: [abc123, def456]`
- Calculates initial blast radius

### Phase 2: RCA Investigation & Blast Radius Expansion

During investigation, agents discover affected services through:
- Application logs (service names in errors)
- Metrics (which services show anomalies)
- Distributed traces (downstream services in call path)
- Deployment history (which services were recently changed)

When agents call `update_affected_services([...services...])`:

```typescript
// Agent adds affected service
agent.update_affected_services(["api-service", "payment-service", "database"])

// System automatically:
// 1. Resolve services to topology nodes
// 2. Query topology edges to find 1-hop neighbors
// 3. Calculate blast_radius
// 4. Update incident.metadata.blast_radius
// 5. Log timeline entry with results
```

**Blast Radius Calculation:**

```
Given pinned nodes: [api-service-node, db-node]

1. Find all edges from pinned nodes
   ├─ api-service → web-frontend (1-hop)
   ├─ api-service → payment-service (1-hop)
   └─ db-node → replica-db (1-hop)

2. Count unique affected nodes: 5
   └─ [api-service, db, web-frontend, payment-service, replica-db]

3. Count edges between any affected nodes: 4

4. Extract domains from affected nodes:
   └─ ["api.example.com", "db.example.com", "web.example.com", ...]

5. Store affected_node_ids[] for visualization
```

### Phase 3: Frontend Visualization

When user views incident RCA analysis:

```
1. IncidentTopologyProvider loads affected_node_ids[] from incident metadata
   ├─ passes incident severity (for color coding)
   └─ wraps topology rendering

2. TopologyTab renders graph with ReactFlow

3. For each ResourceNode:
   ├─ Check: Is this node in affected_node_ids[]?
   ├─ If YES → Apply severity-colored border + ring
   └─ If NO → Fade (opacity 0.3) when "Affected Only" mode active

4. User can toggle "Affected Only" filter
   └─ Focus visualization on impact, de-emphasize unaffected services
```

---

## Incident Topology Fields

### Setting Topology for an Incident

**At Creation Time:**

```bash
curl -X POST https://api.cloudthinker.io/incidents \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "title": "Database Failure",
    "severity": "critical",
    "topology_view_id": "550e8400-e29b-41d4-a716-446655440000",
    "affected_services": ["postgres-primary", "postgres-replica"]
  }'
```

**Via RCA Trigger:**

When starting RCA analysis, the system automatically:
1. Uses `topology_view_id` from incident (if set)
2. Falls back to workspace default (if incident has none)
3. Stores on incident before RCA run creation

**Manual Selection:**

Users can select topology via incident detail page:
- Click **Edit Topology** in incident header
- Select from dropdown of available views
- System auto-detects nodes from incident services

### Pinned Nodes

`pinned_node_ids` stores the primary affected nodes:

```typescript
// Auto-populated by auto_detect_topology()
// Can be manually expanded by agents via update_affected_services()

incident.pinned_node_ids = [
  "node-uuid-1", // Primary affected node
  "node-uuid-2", // Primary affected node
]
```

When topology view is deleted, these nodes provide fallback for visualization.

---

## Service Resolution & Matching

### How Services Resolve to Nodes

The system matches services to topology nodes using configurable logic:

**1. External ID Matching** (Highest Priority)

```python
# Service objects can include external_id from cloud providers
service = {
  "name": "api-service",
  "external_id": "arn:aws:ec2:us-east-1:123456789:instance/i-abc123"
}

# System queries: WHERE external_id = 'arn:aws:ec2:us-east-1:123456789:instance/i-abc123'
# Result: Fast, exact match → node UUID
```

**2. Name Pattern Matching** (Fallback)

```python
# Service with only name
service = {"name": "auth-service"}

# System queries: WHERE name ILIKE '%auth-service%'
# Results:
#   ✅ "auth-service-prod" → MATCH
#   ✅ "auth-service-staging" → MATCH
#   ❌ "auth-cache" → NO MATCH

# If multiple matches: Returns first match (or all and agents choose)
```

**3. Manual Override**

Agents can explicitly pin nodes:

```typescript
agent.update_blast_radius({
  node_ids: ["specific-node-uuid-1", "specific-node-uuid-2"]
})
```

### Best Practices for Matching

To ensure reliable service-to-node resolution:

1. **Use external IDs** - AWS ARNs, Kubernetes resource names, etc.
   - Set `GraphNode.external_id` to cloud provider identifier
   - Service names can then reference via external ID

2. **Consistent naming** - Service names in incidents should match topology node names
   - Use kebab-case: "api-service", not "API_Service" or "api_service"
   - Avoid abbreviations that don't appear in topology

3. **Document mappings** - Create incident templates with known service→node mappings
   - Reduces agent guesswork
   - Improves resolution accuracy on first incident

---

## Blast Radius Visualization

### Frontend Display

The RCA analysis view shows blast radius in a split-pane layout:

**Left Panel (50% default, resizable):**
- Investigation timeline
- Hypothesis tracking
- Evidence chain
- Remediation actions

**Right Panel (50% default):**
- Interactive topology graph
- Affected nodes highlighted with severity color
- "Affected Only" toggle to filter non-affected services
- Affected service count in header

### Severity-Based Coloring

Topology nodes colored by incident severity:

| Severity | Color | Tailwind Class | Meaning |
|----------|-------|----------------|---------|
| **Critical** | Red | `ring-2 ring-red-500 border-red-500` | Service down, high impact |
| **High** | Red | `ring-2 ring-red-500 border-red-500` | Service degraded significantly |
| **Medium** | Blue | `ring-2 ring-blue-500 border-blue-500` | Service degraded moderately |
| **Low** | Blue | `ring-2 ring-blue-500 border-blue-500` | Minor impact |
| **Info** | Gray | `ring-2 ring-gray-500 border-gray-500` | Informational |

### Affected Only Mode

User can toggle "Show All / Affected Only":

**Show All (Default):**
- All topology nodes visible at normal opacity
- Affected nodes have colored borders
- Provides full infrastructure context

**Affected Only (Focused):**
- Non-affected nodes fade (opacity 0.3, grayscale filter)
- Affected nodes at full brightness
- Emphasizes blast radius impact
- Useful for large complex topologies

---

## Agent Investigation with Topology Context

### Context Gathering Phase

During Phase 1, RCA agents:

```
1. Explore infrastructure topology (if topology_view_id set)
   ├─ Identify entry point service (primary failure)
   ├─ Map dependency chain
   └─ Calculate expected blast radius

2. Discover affected services
   ├─ Application errors: "auth-service failed"
   ├─ Cascade detection: "payments timing out → depends on auth-service"
   ├─ Metric analysis: "see degradation in web-frontend, api-service"
   └─ Resolve to topology nodes

3. Update incident metadata
   └─ call update_affected_services([...])
       ├─ Auto-derives blast_radius
       ├─ Stores node IDs
       └─ Logs results in timeline
```

**Timeline Entry:**
```
Finding: "Topology analysis identified 5 affected services"
├── api-service (primary failure)
├── payment-service (1-hop dependent)
├── web-frontend (1-hop dependent)
├── notification-service (1-hop dependent)
└── database (direct dependency of api-service)
```

### Analysis & Hypothesis Phase

Agents form hypotheses with topology context:

```
Hypothesis 1: "Connection pool exhaustion at database"
├── Evidence: Database metrics show 95/100 connections
├── Topology context:
│   └─ [db] has 5 downstream clients
│   └─ If pool exhausted, all 5 degraded simultaneously
└── Prediction: All 5 affected nodes should show timeouts

Hypothesis 2: "API service deployment regression"
├── Evidence: Deployment at 14:22, incident at 14:35
├── Topology context:
│   └─ [api-service] is critical hub
│   └─ Affects 3 immediate dependents + 2 transitive
└── Prediction: Symptom propagation follows dependency chain
```

### Resolution Phase

Final findings with impact assessment:

```
Root Cause: "Memory limit reduced in container config"

Blast Radius Summary:
├── Primary affected: api-service (memory pressure)
├── 1-hop impact:
│   ├── payment-service (timeout waiting for api)
│   ├── web-frontend (503 errors from api)
│   ├── notification-service (queue backlog from api)
│   └── analytics-collector (missed metrics from api)
├── Total nodes: 5
├── Total edges: 7
└── Affected domains:
    ├── api.example.com
    ├── web.example.com
    ├── payments.example.com
    └── notifications.example.com

Remediation:
├── Critical: Revert container memory limit to 512MB
├── High: Review CI/CD config change process
└── Medium: Add memory pressure alerting thresholds
```

---

## Correlation & Alert Grouping

### Topology-Aware Alert Correlation

When multiple alerts arrive, correlation rules can reference topology:

**Rule Example:**
```json
{
  "name": "Same Service Hierarchy",
  "priority": 1,
  "conditions": {
    "operator": "and",
    "items": [
      {
        "field": "affected_services",
        "operator": "has_topology_dependency",
        "topology_view_id": "550e8400-..."
      }
    ]
  }
}
```

Correlates alerts when services are in same dependency chain.

### Time Window Correlation

Groups alerts within time window (default 5 minutes):
- Alerts for api-service + database within 5min → Single incident
- Tracks `correlation_group_id` and `correlated_alert_count`

---

## Configuration

### Workspace Default Topology

Set workspace-level default for all incidents:

**Incidents → Settings → Default Topology View**

```
├─ Select topology view
├─ Apply to all new incidents
├─ Override per-incident as needed
└─ Fallback: No topology (manual selection)
```

### Topology View Selection

When creating incident, users can:

1. **Use workspace default** - Auto-selected if configured
2. **Select specific view** - Override for this incident
3. **None** - Skip topology (investigations won't have topology context)

### Incident Template with Topology

Create reusable templates:

```json
{
  "name": "Database Incident",
  "topology_view_id": "550e8400-...",
  "default_severity": "high",
  "auto_trigger_rca": true,
  "affected_services": ["postgres-primary", "postgres-replica"]
}
```

---

## API Reference

### Creating Incident with Topology

```bash
POST /incidents

{
  "title": "Service Degradation",
  "severity": "high",
  "topology_view_id": "550e8400-e29b-41d4-a716-446655440000",
  "affected_services": ["api-service", "database"]
}
```

**Response:**
```json
{
  "id": "incident-uuid",
  "title": "Service Degradation",
  "topology_view_id": "550e8400-...",
  "pinned_node_ids": ["node-1", "node-2"],
  "metadata": {
    "blast_radius": {
      "node_count": 5,
      "edge_count": 7,
      "affected_node_ids": ["node-1", "node-2", "node-3", "node-4", "node-5"],
      "affected_domains": ["api.example.com", "web.example.com", ...]
    }
  }
}
```

### Updating Affected Services

During RCA investigation:

```bash
POST /incidents/{incident_id}/tools/incident/update_affected_services

{
  "affected_services": ["api-service", "payment-service", "database"]
}
```

**System automatically:**
1. Resolves services to topology nodes
2. Calculates new blast radius
3. Updates metadata.blast_radius
4. Returns results with affected node count

---

## Limitations & Constraints

### Node Limit

Auto-detection limited to **10 nodes maximum**:
- Prevents runaway queries on large topologies
- Agents can manually expand via explicit node selection
- Rule: First 10 matching nodes by relevance score

### Topology View Requirement

Blast radius calculation requires **active topology view**:
- If topology view deleted → System falls back to `pinned_node_ids`
- If no pinned nodes → No visualization
- Recommendation: Archive topology views instead of deleting

### Service Name Matching

Pattern matching returns **first match only** when multiple nodes match:
- "api-service" might match ["api-service-prod", "api-service-staging"]
- Recommendation: Use unique service names or external IDs

---

## Best Practices

### Setup

1. **Configure workspace default topology**
   - Incidents → Settings → Default Topology View
   - Reduces manual selection on each incident

2. **Use external IDs** on topology nodes
   - Set GraphNode.external_id to cloud provider identifiers (ARNs)
   - Improves service resolution accuracy

3. **Create incident templates** with known topology associations
   - Template for "database incidents" → pre-set topology + services
   - Speeds incident creation

### During Investigation

4. **Monitor blast radius growth**
   - Review affected nodes as investigation progresses
   - Expanding blast radius indicates broader impact

5. **Verify node mappings**
   - Check that pinned_node_ids make sense for services
   - Manually expand if agents missed dependencies

6. **Use "Affected Only" mode**
   - Focus visualization on impact for large topologies
   - Easier to trace dependency chains

### Post-Investigation

7. **Document incident topology patterns**
   - "Database outages affect X, Y, Z services"
   - Use for alert correlation rules

8. **Review topology for accuracy**
   - Ensure edges reflect actual dependencies
   - Update if infrastructure changes

---

## Troubleshooting

| Issue | Cause | Solution |
|-------|-------|----------|
| **Services not found in topology** | Service names don't match node names | Use external_id matching or update topology node names |
| **Blast radius looks incomplete** | Limited to 10 nodes in auto-detection | Manually add services via update_affected_services |
| **Topology not displaying** | topology_view_id not set | Configure workspace default or select per-incident |
| **Wrong nodes highlighted** | Service name matches multiple nodes | Use external_id or make service names unique |
| **Affected Only filter not working** | No affected_node_ids in metadata | Create RCA run to auto-calculate blast radius |

---

## See Also

- [Root Cause Analysis](/incident/root-cause-analysis) - Investigation workflow using topology context
- [Topology Fundamentals](/topology/overview) - Building and managing topology views
- [Webhook Integrations](/incident/webhook-integrations) - Auto-create incidents from alerts
